{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c514cad",
   "metadata": {},
   "source": [
    "# Limpieza y Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ac4e9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/housing.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638dcff6",
   "metadata": {},
   "source": [
    "Aquí Realizaremos la inputación de mediana a aquellos valores faltantes para no perder información y reemplazarlos. Como se muestra ya no hay nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "382eb23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "abc1b509-4480-41ad-ad1f-9de4198be65f",
       "rows": [
        [
         "longitude",
         "0"
        ],
        [
         "latitude",
         "0"
        ],
        [
         "housing_median_age",
         "0"
        ],
        [
         "total_rooms",
         "0"
        ],
        [
         "total_bedrooms",
         "0"
        ],
        [
         "population",
         "0"
        ],
        [
         "households",
         "0"
        ],
        [
         "median_income",
         "0"
        ],
        [
         "median_house_value",
         "0"
        ],
        [
         "ocean_proximity",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "longitude             0\n",
       "latitude              0\n",
       "housing_median_age    0\n",
       "total_rooms           0\n",
       "total_bedrooms        0\n",
       "population            0\n",
       "households            0\n",
       "median_income         0\n",
       "median_house_value    0\n",
       "ocean_proximity       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df[\"total_bedrooms\"].fillna(df[\"total_bedrooms\"].median())   # Aquí elegí inputar datos faltantes a removerlos\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f56bc",
   "metadata": {},
   "source": [
    "Aquí prepararemos los conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67b7fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"median_house_value\", axis=1)\n",
    "y = df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac59eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca4e90",
   "metadata": {},
   "source": [
    "Posteriormente, se construirá un pipeline de preprocesamiento en el que primero se separarán las variables numéricas y categóricas para aplicarles transformaciones específicas.\n",
    "\n",
    "A las variables numéricas se les incorporará la imputación por mediana (incluida dentro del pipeline como buena práctica), se aplicará transformación logarítmica a aquellas con asimetría positiva identificada en el EDA y finalmente se realizará el escalamiento para estandarizar su magnitud.\n",
    "\n",
    "En el caso de las variables categóricas, se llevará a cabo la imputación de valores faltantes y su codificación mediante One Hot Encoding para representarlas en formato numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fea7b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\"\n",
    "]\n",
    "categorical_features = [\"ocean_proximity\"]\n",
    "\n",
    "log_features = [\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\"\n",
    "]\n",
    "\n",
    "num_no_log = list(set(numeric_features) - set(log_features))\n",
    "\n",
    "log_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log_transform\", FunctionTransformer(np.log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer([\n",
    "    (\"num_log\", log_pipeline, log_features),\n",
    "    (\"num\", num_pipeline, num_no_log),\n",
    "    (\"cat\", cat_pipeline, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0867c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test_prepared = preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1e0bf2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    # esta función la incluyo para limpiar cualquier bracket que haya quedado en el encoding al pasarlo a df\n",
    "def clean_col_names(df):\n",
    "    df.columns = [re.sub(r\"[\\[\\]<>]\", \"\", col) for col in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "91102514",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = preprocessing_pipeline.get_feature_names_out()\n",
    "\n",
    "X_train_prepared = clean_col_names(pd.DataFrame(X_train_prepared, columns=col_names))\n",
    "X_test_prepared  = clean_col_names(pd.DataFrame(X_test_prepared,  columns=col_names))\n",
    "\n",
    "\n",
    "# guardo los conjuntos de datos para ocuparlos en el script del modelo\n",
    "X_train_prepared.to_parquet(\"../data/X_train_prepared.parquet\", index=False)\n",
    "X_test_prepared.to_parquet(\"../data/X_test_prepared.parquet\",  index=False)\n",
    "\n",
    "pd.Series(y_train, name=\"median_house_value\").to_frame().to_parquet(\"../data/y_train.parquet\", index=False)\n",
    "pd.Series(y_test,  name=\"median_house_value\").to_frame().to_parquet(\"../data/y_test.parquet\",  index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (juan)",
   "language": "python",
   "name": "juan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
